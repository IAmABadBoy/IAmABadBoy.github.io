<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="oooo的博客" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="oooo的博客">
<meta property="og:type" content="website">
<meta property="og:title" content="oooo的博客">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="oooo的博客">
<meta property="og:description" content="oooo的博客">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>oooo的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">oooo的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">副标题</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/04/Mono-SF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="oooo">
      <meta itemprop="description" content="oooo的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="oooo的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/04/Mono-SF/" class="post-title-link" itemprop="url">Mono-SF:Multi-ViewGeometry_Meets_Single-View_Depth_for_Monocular_Scene_Flow_Estimation_of_Dynamic_Trafﬁc_Scenes</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-12-04 09:58:15 / 修改时间：16:29:24" itemprop="dateCreated datePublished" datetime="2019-12-04T09:58:15+08:00">2019-12-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">文章阅读</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h2><blockquote>
<ul>
<li>Abstract</li>
<li>Introduction</li>
<li>Related Work</li>
<li>Methods</li>
<li>Experiments</li>
<li>Conclusion</li>
</ul>
</blockquote>
<h2 id="文章解读"><a href="#文章解读" class="headerlink" title="文章解读"></a>文章解读</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>&emsp; &emsp;现有的的3D场景流估计方法提供了场景的3D geometry和3d motion。traditionally，这些方法基于stereo images 的时间序列我们提出了novel单目3D场景流估计方法——MonoSF。它通过组合multi-view geometry and single-view depth information，共同估算场景的3D结构和运动。为了以统计方式集成单试图深度，我们提出了一个ProbDepthNet的卷积神经网络。它从整个图片来估计像素的深度分布而不是单个深度值。此外，我们还提出了一个新颖的用于回归问题的recalibration technique来确保well-calibrated distributions。</p>
<p>calibrate 校准</p>
<p>Keypoints：</p>
<blockquote>
<ul>
<li><p>MonoSF-组合multi-view geometry and single-view depth information来估算3D结构和运动</p>
</li>
<li><p>ProbDepthNet卷积神经网络</p>
</li>
<li><p>novel recalibration technique </p>
</li>
</ul>
</blockquote>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>&emsp;&emsp;单目系统VS双目系统：</p>
<blockquote>
<ul>
<li>单目系统更便宜，避免了双目系统中的校准工作。</li>
<li>在单目系统设置中，3D 场景流估计是一个ill-posed（不确定）问题。</li>
<li>为了解决歧义，以前的方法假定运动对象与周围环境接触或场景的表面和运动遵循一个平滑度。但是假设可能会被违背。</li>
<li>单目系统里有一些方法可以从单个图像中提供深度估计。我们在概率优化框架中将多视图几何和单视图深度信息结合起来，提供一致的3D场景流估计。单视图深度用于解决多视图几何的歧义。</li>
</ul>
</blockquote>
<p>&emsp;&emsp;之前的方法表明，对特定交通场景的suitable表示是分解成3D planar surface elements，每个element都被视作一个rigid body（刚体）。遵循此规律，MonoSF联合考虑以下因素，共同估计每个平面的3D几何形状和每个刚体的6D运动：</p>
<blockquote>
<ul>
<li>通过将参考图片扭曲成consecutive图像的多视图几何</li>
<li>概率单视图深度估计</li>
<li>场景模型的平滑度检验</li>
</ul>
</blockquote>
<p>另外，利用instance分割来检测潜在移动对象的集合。</p>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>略</p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>将从两方面进行介绍—-ProbDepthNet和MonoSF模型和优化框架</p>
<h4 id="Probabilistic-Single-View-Depth-Estimation"><a href="#Probabilistic-Single-View-Depth-Estimation" class="headerlink" title="Probabilistic Single-View Depth Estimation"></a>Probabilistic Single-View Depth Estimation</h4><p>&emsp;&emsp;它的主要目的是为了提供一个像素密度概率函数。深度值被编码为它的深度值倒数形式 $d=Z^{-1}$,其中Z是摄像机坐标中3D位置的Z坐标，ProbDepthNet估计参数化为高斯混合的像素级概率密度函数$p_\mathbf{P}(d|I)$</p>
<p><img src="/2019/12/04/Mono-SF/3.png" alt="3"></p>
<p>其中 K 代表 the num of components，$\lambda_i$是权重，$\mu_i$代表平均值，$\sigma_i$代表第i个分量的方差。</p>
<p><img src="/2019/12/04/Mono-SF/2.png" alt="2"></p>
<blockquote>
<ul>
<li>ProbDepthNet 由两部分组成：DepthNet 和CalibNet。DepthNet是一个full卷积的ResNet-50，在相应的编码器和解码器层之间有skip connection。DepthNet的输出是高斯混和的参数，凭借在对数空间中提供方差 $s_i =\log(\sigma_i)$。</li>
<li>其次，方差$ s_i$ 和权重$\lambda_i$ 将由CalibNet进行重新校准。 CalibNet由5个1*1的卷积层组成：没有非线性激活函数的第一层将输入按比例缩放。剩余的4层包括作为激活函数的指数线性单位。</li>
<li>两个网络在训练数据的不同分割上进行训练，以避免DepthNet在校准分割上过度拟合。</li>
</ul>
</blockquote>
<p>负的对数似然函数 loss $\mathcal{L}$在训练期间损失最小：</p>
<p><img src="/2019/12/04/Mono-SF/4.png" alt="4"></p>
<p>为了克服激光雷达数据在密度、视野、方面的局限性，基于双目图像的中间融合被用做真实深度的生成。</p>
<blockquote>
<ul>
<li>首先，将激光点云投射到图片上并去除不一致的测量值以处理遮挡问题。</li>
<li>其次，考虑到两个立体图像之间的光度距离，可以使用基于SGM的方法来补全稀疏深度图</li>
</ul>
</blockquote>
<p>ProbDepthNet通过在训练过程中观察深度分布来学习估计像素级深度分布。 因此，深度分布捕捉了有关Kendall和Gal理论的不确定性。 无视不确定性被认为是许多视觉应用中最主要的不确定性[32]。</p>
<h4 id="Monocular-Scene-Flow"><a href="#Monocular-Scene-Flow" class="headerlink" title="Monocular Scene Flow"></a>Monocular Scene Flow</h4><h5 id="Monocular-Scene-Flow-Model"><a href="#Monocular-Scene-Flow-Model" class="headerlink" title="Monocular Scene Flow Model"></a>Monocular Scene Flow Model</h5><p>&emsp;&emsp;主要假设是，可以通过一组 piecewise planar 平面元素来表示场景结构，一组刚体来代表运动。正式地，参考图像被划分为一组超像素，每个超像素代表一个3D平面，每个3D平面均由其法线$n_i\in \mathbf{R}^3$定义并按该平面到相机的距离的倒数缩放。并且以$n^T_iX=1$来编码平面上每个3D点X的坐标。刚体组包括背景和其他交通参与者，比如被instance segmentation检测到的人和车。即使行人在一定程度上没有进行刚体运动，也可以通过主导刚体变换来近似。每个刚体由其6D运动$T_j\in SE(3)$表示。另外，每一个超像素与一个刚体以及对应的超像素的像素$R_i$关联。</p>
<p><img src="/2019/12/04/Mono-SF/5.png" alt="5"></p>
<h5 id="Energy-Minimization-Problem"><a href="#Energy-Minimization-Problem" class="headerlink" title="Energy Minimization Problem"></a>Energy Minimization Problem</h5><p>&emsp;&emsp;Mono-SF的主要思想是场景几何形状和运动在扭曲连续图像$I_1$ 中的参考图像$I_0$ 方面应保持一致，并与ProbDepthNet提供的深度分布$p(d|I_0)$和$p(d|I_1)$保持一致.形式上，作为能量最小问题，Mono-SF 共同优化了每个刚体的6D运动和每个平面$n_i$ 的3D法线。能量项E由每个像素$p_0$的一元数据项$\Phi(p_0,n_i,T_j)$和相邻图片$k,l\in \mathbf{N}$中的每两个平面$n_k$和$n_l$组成的成对平滑度项$\Psi(n_i,n_j)$。</p>
<p><img src="/2019/12/04/Mono-SF/6.png" alt="6"></p>
<p>$\Phi(p_0,n_i,T_j)$由两部分组成。</p>
<p><img src="/2019/12/04/Mono-SF/7.png" alt="7"></p>
<blockquote>
<ul>
<li>$\Phi^{pho}(p_0,n_i,T_j)$最小化像素$p_0$和其在连续图像中的投影位置的基于外观的测光距离</li>
<li>$\Phi^{svd}_t(p_0,n_i,T_j)$偏爱在t=0和1时的ProbDepthNet的估计深度概率一致的3D位置。</li>
</ul>
</blockquote>
<p><img src="/2019/12/04/Mono-SF/9.png" alt="9"></p>
<p>Census descriptors？？？   decomposition of  。。。的分解</p>
<p><img src="/2019/12/04/Mono-SF/image-20191204160731631.png" alt="image-20191204160731631"></p>
<p><img src="/2019/12/04/Mono-SF/image-20191204161001626.png" alt="image-20191204161001626"></p>
<blockquote>
<p>场景模型先验以成对平滑度的形式表示。</p>
<p>$\Psi^d(n_k,n_l)$表示深度上的平滑</p>
<p>$\Psi^{ori}(n_k,n_l)$表示方向上的平滑</p>
</blockquote>
<p><img src="/2019/12/04/Mono-SF/image-20191204161355893.png" alt="image-20191204161355893"></p>
<blockquote>
<p>两个平滑度项都被τ1或τ2截断，以考虑深度或方向的不连续性，例如不同对象之间的不连续性。 超参数Θ和τ根据刚体类型，背景或对象来不同地定义，并且对于属于不同刚体的相邻平面也不同地定义。 为便于阅读，这些关系在前面的公式中被忽略。</p>
</blockquote>
<p><img src="/2019/12/04/Mono-SF/image-20191204161803456.png" alt="image-20191204161803456"></p>
<blockquote>
<p>优化问题需要把所有变量进行合适的初始化。这里应用的关键思想是集成单视图深度信息以提供度量标准。我们将此思想另外应用于运动物体的比例感知姿势估计。首先，通过稀疏流对应关系（pi 0，pi 1），使用简单的投票方案，将由Mask R-CNN 检测到的图像I0和I1中的对象实例配对。每个对象实例以及背景都会构建一个刚体。其次，通过将上式最小化，将每个刚体的6D运动Tj∈SE（3）与一组3D点Xi∈X（对应于对应实例蒙版中的每个流对应关系一个）一起优化。</p>
</blockquote>
<p><img src="/2019/12/04/Mono-SF/image-20191204162047231.png" alt="image-20191204162047231"></p>
<p>analogously  近似地</p>
<p>随后，将这组3D平面初始化。 首先，基于适合单眼情况的半全局匹配来计算密集深度图。 再次，深度估计值由ProbDepthNet估计值附加评估。 其次，使用[68]中的方法初始化包括其3D法线ni的超像素。 平面的像素被强制具有相同的实例，从而获得与刚体的唯一关联。</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p><img src="/2019/12/04/Mono-SF/image-20191204162227596.png" alt="image-20191204162227596"></p>
<p><img src="/2019/12/04/Mono-SF/image-20191204162235349.png" alt="image-20191204162235349"></p>
<p><img src="/2019/12/04/Mono-SF/mage-20191204162254493.png" alt="image-20191204162254493"></p>
<p><img src="/2019/12/04/Mono-SF/image-20191204162305203.png" alt="image-20191204162305203"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="oooo">
      <meta itemprop="description" content="oooo的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="oooo的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/" class="post-title-link" itemprop="url">Multi-View_Stereo_by_Temporal_Nonparametric_Fusio</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-12-03 14:38:48 / 修改时间：20:55:27" itemprop="dateCreated datePublished" datetime="2019-12-03T14:38:48+08:00">2019-12-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">文章阅读</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h2><blockquote>
<ul>
<li>Abstract</li>
<li>Introduction</li>
<li>Related Work</li>
<li>Methods</li>
<li>Experiments</li>
<li>Discussion and conclusion</li>
</ul>
</blockquote>
<h2 id="文章解读"><a href="#文章解读" class="headerlink" title="文章解读"></a>文章解读</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>&emsp;&emsp;我们提出了什么样的模型? </p>
<blockquote>
<p>我们提出了一种新颖的方法，它能从多视角image-pose pairs 中进行深度估计，其中模型还具有利用场景先前隐藏空间编码信息（ information from previous latent-space encodings of the scene）的能力。</p>
<p>模型使用成对的images-poses进行训练，他们通过encoder-decoder model 来进行视差估计。</p>
</blockquote>
<p>&emsp;&emsp;模型的创新点在哪里?</p>
<blockquote>
<p>通过a nonparametric Gaussian process prior 来软约束 the bottleneck layer。</p>
<p>提出了pose-kernel 结构,该结构encourages 相似的 poses  有类似的隐藏空间。</p>
<p>GP（Gaussian process ） prior的灵活性提供了adapting memory来融合之前views的信息。</p>
<p>end-to-end地共同训练encoder-decoder和GP超参数</p>
<p>还导出了一个能实时运行的轻型方案</p>
</blockquote>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>&emsp;&emsp;什么是MVS问题？它有哪些变体和应用？</p>
<blockquote>
<p>MVS是指从具有已知姿势和内部参数的相机拍的图片中重建出3D场景结构的研究</p>
<p>变体：</p>
<ul>
<li>当motion已知时，从单目相机拍的视频中进行深度图估计</li>
<li>用传统双目惯导设备进行深度估计</li>
<li>从图片集中建立image-based3D模型</li>
</ul>
<p>应用：</p>
<ul>
<li>image-based 3D models  能用来测量和可视化大型环境来辅助设计和planning</li>
<li>深度估计对自主机器的感知和SLAM有利</li>
</ul>
</blockquote>
<p>&emsp;&emsp;我们的工作：</p>
<blockquote>
<p>我们的工作主要是对运动已知的单目相机拍的视频进行深度估计，In practice，运动可以用vio（visual-inertial odometry ）进行估，它能实时提供高精度的相机poses，漂移很小并且在手机上可以运行。</p>
</blockquote>
<p>&emsp;&emsp;为什么要做基于视频帧的深度估计而不是基于双目惯导设备？</p>
<blockquote>
<ul>
<li>在小型移动设备中，两个摄像头的baseline不能大，这限制了深度测量的范围。使用单目相机，其运动通常提供了一个比设备尺寸更大的baseline，这将提高远距离区域的测量精度</li>
<li>当相机在空间中旋转跳跃时，通常会从多个连续变化的views观察到相同的场景区域，因此能够有效融合所有这些信息来进行robust和stable的深度估计是有益的。</li>
</ul>
</blockquote>
<p>&emsp;&emsp;我们的方法：</p>
<blockquote>
<ul>
<li>我们提出了一种结合视差估计网络的新方法，该网络具有编码-解码器结构和plane-sweep cost量作为输入，还有一个GP在前面，which 软约束了bottleneck layer 来融合机油相似pose的视频帧信息。</li>
<li>a pose-kernel struct，为了有效改善重叠视图中信息的融合，与时间上的分离无关</li>
<li>既可用于batch mode 也可以 online mode，仅前面的帧会影响当前帧的预测</li>
</ul>
</blockquote>
<p>&emsp;&emsp;我们的贡献：</p>
<blockquote>
<ul>
<li>提出了一个MVS的新颖的方法，它将通过来自隐藏空间的先验概率传递先前重建的深度图信息。</li>
<li>对于non-parametric 隐藏空间，我们提出了pose-kernel方法，用于对观测帧之间影响相机姿态的先验知识进行编码。</li>
<li>证明了CNN encoder-decoder 结构和GP超参数可以一起训练</li>
<li>拓展了我们的方法使其可以在只能手机平板电脑中实时运行。</li>
</ul>
</blockquote>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>&emsp;&emsp;略</p>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><h4 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1. 网络结构"></a>1. 网络结构</h4><p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/1.png" alt="1"></p>
<p><center>图1 网络结构图</center></p>
<blockquote>
<ul>
<li>我们方法由两部分组成</li>
<li>（图中的垂直数据流）CNN驱动的MVS方法，它将输入帧转变成成本量，然后通过编码器-解码器模型产生视差图（深度的倒数）。</li>
<li>（图中的水平数据流）通过在相机轨迹上传递有关隐藏空间的信息来耦合每个独立的视差预测任务。</li>
</ul>
</blockquote>
<p> &emsp;&emsp;对每个image-pair对，我们计算size为D*H*W的成本量，并连接参考的RGB图像作为编码器的输入，本文中的Img尺寸为320*256并且在0.5m-50m的inverse depth中均匀采样了D=64个深度平面。为了计算成本量，我们使用planar homography通过邻近平行帧在固定深度处将平行帧扭曲到参考帧。</p>
<script type="math/tex; mode=display">\mathbf{H}=\mathbf{K}(\mathbf{R}+t(\begin{matrix}0&0&\frac{1}{d_i}\end{matrix} ))\mathbf{K}^{-1}</script><p>$\mathbf{K}$是已知的intrinsics matrix,$(\mathbf{R},\mathbf{t})$是根据旋转矩阵相对于相邻帧的平移矢量给出的，$d_i$表示第i个虚拟平面的深度值，warped相邻帧与参考帧之间的绝对强度差被计算为每个深度平面上每个像素的代价：</p>
<script type="math/tex; mode=display">\mathbf{V}(d_i)=\sum_{R,G,B}{\widetilde{I_{d_i}}-I_r}</script><p>其中 $\widetilde{I_{d_i}}$ 代表通过 $d_i$ 深度平面的warped image，$I_r$代表参考帧。</p>
<p>在编码器中，由5个卷积层，编码后，我们得到一个大小为512*8*10的隐藏空间表示y，它将由GP模型转换得到，转换后的隐藏空间z表示，z作为解码端的输入，生成1*H*W的预测，编码器解码器之间有4个skip connection。所有卷积层之后都进行了Batch Normalization 和 Relu处理，预测层使用的sigmoid函数缩放了2来限制预测范围。为了支持任意输入，我们分别计算了每个相邻图像的成本量，然后对成本量求平均，再将其传到编码器-解码器网络。</p>
<p>在训练过程中，使用N个输入帧序列，我们通过将前一帧用作相邻帧（第一个帧将下一帧用作相邻帧）来预测N个深度图，并使用L1误差的均值 （at four scales）作为训练模型的总体损失。</p>
<h4 id="2-Pose-Kernel-Gaussian-process-prior"><a href="#2-Pose-Kernel-Gaussian-process-prior" class="headerlink" title="2. Pose-Kernel Gaussian process prior"></a>2. Pose-Kernel Gaussian process prior</h4><p>&emsp;&emsp;我们试图在潜在空间上定义一个概率先验来解释先验知识，即具有接近或重叠视野的pose应该比那些互相远离的pose或对着的pose产生更相似的隐藏空间编码。这个知识通过协方差（kernel）编码，因此我们需要定义距离度量或者在pose空间定义closeness的度量。</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/4.png" alt="4"></p>
<p>其中姿势定义为$P=\{t,R\}$ 属于$R^3\times{SO(3)}$,<strong>I</strong>是一个单位矩阵，tr代表矩阵trace operator。</p>
<p>kernel设计：</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/5.png" alt="5"></p>
<p>这个内核将两个任意相机姿态P，P′编码为nearness或者similarity in latent space。可学习超参数$\gamma^2$和ℓ定义了过程的特征量级和长度尺寸。</p>
<p>为了在帧中共享时间信息，我们为$z_i$中的所有值分配了独立的GP先验，并认为编码器输出的$y_i$是受噪声干扰的版本，该推断问题可以表示为一下GP回归模型：</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/6.png" alt="6"></p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/7.png" alt="7"></p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/2.png" alt="2"></p>
<h4 id="Latent-state-batch-estimation"><a href="#Latent-state-batch-estimation" class="headerlink" title="Latent-state batch estimation"></a>Latent-state batch estimation</h4><p>&emsp; &emsp;似然是高斯函数，并且所有的GP共享相同的poses来评估协方差函数，因此我们可以使用一个矩阵求逆来解决512*8*10的 GP回归问题。这是由于后验协方差仅是输入姿势的函数，而不是图像的学习表示的值。</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/8.png" alt="8"></p>
<p>covariance matrix  协方差矩阵</p>
<p>该批处理方案考虑了序列中所有相互关联的姿势，使其功能强大。 不利的一面是矩阵C随着输入帧/姿势的数量N的增长而增长，并且推理需要将矩阵求逆-矩阵大小按立方缩放。 因此，该方案仅适用于数百个帧的序列。</p>
<h4 id="Online-Estimation"><a href="#Online-Estimation" class="headerlink" title="Online Estimation"></a>Online Estimation</h4><p>&emsp;&emsp;当输入的图像姿势对有了时间顺序属性后，我们可以吧模型简化成一个directed graph。这种情况下，GP推理问题可以变成状态空间形式解决，每个姿态-帧的计算具有恒定的时间空间复杂度。我们通过下面过程可以精确地推论而无需近似。</p>
<p>&emsp;&emsp; 对于状态空间GP的推断，将协方差函数转换为dynamical模型， 选择初始状态作为协方差函数相对应的稳态</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/9.png" alt="9"></p>
<p>我们共同推断所有独立GP的后验，以使其均值$u_i$ 为大小2*（512*8*10）的矩阵，其中列是 独立GPs的time-marginal means，二维状态来自于matern模型。协方差矩阵在所有的独立GP见共享。这使得推理很快。</p>
<p>我们定义了一个具有matern性质的演化算子</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/10.png" alt="10"></p>
<p> consecutive  连贯的    propagated 传播</p>
<p>通过对当前step编码器输出$y_i$ 进行调节来给出后验均值和协方差:</p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/11.png" alt="11"></p>
<p> overloaded notation 符号重载       derivatives 导数</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/image-20191203205232123.png" alt="image-20191203205232123"></p>
<p><img src="/2019/12/03/Multi-View-Stereo-by-Temporal-Nonparametric-Fusio/image-20191203205339972.png" alt="image-20191203205339972"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/02/Group-wise-Correlation-Stereo-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="oooo">
      <meta itemprop="description" content="oooo的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="oooo的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/02/Group-wise-Correlation-Stereo-Network/" class="post-title-link" itemprop="url">Group-wise-Correlation-Stereo-Network</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-02 15:27:46" itemprop="dateCreated datePublished" datetime="2019-12-02T15:27:46+08:00">2019-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-12-03 14:19:02" itemprop="dateModified" datetime="2019-12-03T14:19:02+08:00">2019-12-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">文章阅读</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h2><pre><code> 0. Abstract
 1. Introduction
 2. Related Work
 3. Group-Wise Correlation Network
 4. Experiment
 5. Conclusion
</code></pre><h2 id="文章解读"><a href="#文章解读" class="headerlink" title="文章解读"></a>文章解读</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>&emsp;&emsp;以前的工作中构建了跨越所有disparity levels的左右特征的 cross-correlation or concatenation的cost volumes，然后用2d或3d卷积神经网络来回归视差图。本文提出了用分组的方法来构建cost volumes。沿着channel dimension 左右特征被分成小组，然后在组之间计算correlation map，得到多匹配的cost proposals，然后打包成 cost volume。Group-wise 相关提供了度量特征相似性的有效表示，并且不会像full correlation丢失太多信息。</p>
<p>&emsp;&emsp;<a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">代码链接</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>&emsp;&emsp;<strong>传统的双目流程通常包含4步：</strong></p>
<ul>
<li>matching cost computation</li>
<li>cost aggregation</li>
<li>disparity optimization</li>
<li>postprocessing</li>
</ul>
<p>matching cost computation 是为了给左右图patches提供初始相似性度量。常见的matching cost 包括 绝对差（SAD）、平方差之和（SSD）、归一化互相关（NCC）</p>
<p>cost aggregation 和 optimization 包括上下文匹配成本和先验知识来得到更可靠的disparity预测</p>
<p>learning-base 的方法利用了不同的特征表示和聚类方法来计算matching costs。列举了几个网络和各自的方法。</p>
<p><strong>他们的缺点：</strong></p>
<blockquote>
<p>full correlation 丢失了很多信息因为它对每个disparity level只提供了一个单通道的相关图</p>
<p>concatenation volume 需要更多的参数从头学习相似性度量功能</p>
<p>hand-crafts的还在用传统的匹配成本，不能优化成端到端网络</p>
</blockquote>
<p><strong>我们的方法：</strong></p>
<blockquote>
<p>提取多级一元特征来构成左右图像对的高维度特征表示  fl , fr 。然后将特征沿着channel dimension进行分组。在所有disparity levels，第 i 个左特征组将与第 i 个右特征组相关联以获得group-wise的相关图，最后将所有相关图打包成一个4D成本量。</p>
<p>一元特征可视为groups of structed vectors，因此一个组的correlation maps 可以视为一个matching cost proposal </p>
</blockquote>
<p><strong>我们的主要贡献：</strong></p>
<blockquote>
<ul>
<li>提出了group-wise方法来构建cost volumes，提供了更好的相似性度量</li>
<li>改善了 the stacked 3D hourglass refinement network</li>
<li>在3个数据集上，比以前的方法有更好的性能</li>
<li>在限制聚合的计算成本时，性能降低小，在实时双目网络中有应用价值。</li>
</ul>
</blockquote>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>&emsp;&emsp;略</p>
<h2 id="Group-wise-Correlation-Network"><a href="#Group-wise-Correlation-Network" class="headerlink" title="Group-wise Correlation Network"></a>Group-wise Correlation Network</h2><p>&emsp;&emsp;GwcNet是基于PSMNet，加入了group-wise correlation cost volume 和改进了 3d stacked hourglass network。</p>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/1.png" alt="1"></p>
<center>Figure 1 GwcNet 流程图</center>
&emsp;&emsp;网络包含4个部分：一元特征提取、cost volume construction、 3d卷积聚合、 disparity预测。

> + cost volume 包含两个部分，concatenation volume 和group-wise correlation volume。
>
> + concatenation volume 是通过串联压缩后的左右特征来构建的
>
> + group-wise correlation volume 如下图所示



![2](2.png)

<center>Figure 2 3D聚合网络结构</center>



<p>&emsp;&emsp;<strong>网络前面是4个卷积，后面是3个堆叠的3d沙漏网络。</strong></p>
<p>&emsp;&emsp;特征提取采用了PSMNet里的ResNet-like 网络，它带有半拓展性，并且去掉了spatial pyramid pooling module。最后的conv2、conv3、conv4的特征图会被串成320-channel的一元特征图。</p>
<p>&emsp;&emsp;cost volume 由两部分组成：  a concatenation volume 和 a group-wise correlation volume。一元特征图会经过两次卷积压缩到12 channels到达concatenation volume。g-wcv会在下面具体讲到。这两个量会串联起来作为3D aggregation 网络的输入。</p>
<p>&emsp;&emsp;3D 聚合网络用来聚合相邻像素和视差的特征。如图2所示，前面的模块由4个具有 batch normalization and ReLU的3d卷积层组成。跟随的3个堆叠的3d沙漏网络通过encoder-decoder结构来修正低纹理和遮挡的情况。几个改进在下面会详细讲到。</p>
<h4 id="Group-wise-correlation-volume"><a href="#Group-wise-correlation-volume" class="headerlink" title="Group-wise correlation volume"></a>Group-wise correlation volume</h4><p>&emsp;&emsp;之前工作的问题：</p>
<blockquote>
<ul>
<li>cost volume是由不同视差level的左右特征关联或串联组成，它们都有缺点。</li>
<li>全关联提供了有效的方式来衡量特征相似度，但是丢失了很多信息，因为每个视差level它只产生了单通道的关联图。</li>
<li>串联量不包含特征相似的信息，因此网络需要更多参数来头学习特征相似度度量。</li>
</ul>
</blockquote>
<p>&emsp;&emsp;我们的group-wise correlation通过结合关联和串联量来解决问题。</p>
<script type="math/tex; mode=display">
\mathbf{C}_{gwc}(d,x,y,g)= \frac{1}{N_c/N_g}<\mathbf{f}^g_l(x,y),\mathbf{f}^g_r(x-d,y)></script><blockquote>
<ul>
<li>基本思想是把特征分组并且以组为单位计算correlation maps。</li>
<li>&lt;*,*&gt;表示内积，N<sub>c</sub>为一元特征的通道数，N<sub>g</sub>为组数，<strong>f</strong><sup>g</sup><sub>r</sub>为第g组右边的特征组，d为视差level。</li>
<li>然后所有的相关图打包为[D<sub>max</sub>/4, H/4, W/4, N<sub>g</sub>]形式的cost量。D<sub>max</sub>代表最大视差，D<sub>max</sub>/4代表特征最大视差。当N<sub>g</sub>为1时，分组关联就变成了全关联。</li>
</ul>
</blockquote>
<h4 id="Improved-3D-aggregation-module"><a href="#Improved-3D-aggregation-module" class="headerlink" title="Improved 3D aggregation module"></a>Improved 3D aggregation module</h4><p>&emsp;&emsp;我们对堆叠的沙漏架构进行了修改使其适合我们提出的分组相关性并提高了推理速度。</p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/3.png" alt="3"></p>
<p><center>Figure 3 3D聚合结构表</center></p>
<blockquote>
<ul>
<li>在沙漏前模块之后添加了一个输出模块（Output Module 0）</li>
<li>移除了不同输出模块之间的剩余连接，因此可在推理期间移除辅助输出模块（0,1,2）</li>
<li>将1*1*1的3D卷积添加到每个沙漏模块内的快捷方式连接，图2中的虚线，在不增加大量计算成本的情况下提高性能。</li>
</ul>
</blockquote>
<h4 id="Output-module-and-loss-function"><a href="#Output-module-and-loss-function" class="headerlink" title="Output module and loss function"></a>Output module and loss function</h4><p>&emsp;&emsp;对于每个输出模块，采用两个3D卷积生成一个单通道的4D量，然后将这个量进行上采样并转换为沿视差维具有softmax性质的概率量。对于每个像素都有一个D<sub>max</sub> 长度的向量，其中包含所有视差level的概率p。然后视差估计为：</p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/4.png" alt="4"></p>
<p>其中k代表可能的视差level，p<sub>k</sub>代表对应的概率。</p>
<p>输出的4个视差预测图表示为：</p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/7.png" alt="7"></p>
<p>最后的loss表示为：</p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/5.png" alt="5"></p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/8.png" alt="8"></p>
<p>(打不出来那个数学符号就贴图了)。</p>
<p>coefﬁcients 系数</p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/6.png" alt="6"></p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/9.png" alt="9"></p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/10.png" alt="10"></p>
<p><img src="/2019/12/02/Group-wise-Correlation-Stereo-Network/11.png" alt="11"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>&emsp;&emsp;略。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">oooo</p>
  <div class="site-description" itemprop="description">oooo的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/iamabadboy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;iamabadboy" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">oooo</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
